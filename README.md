<div align="center">
  <h1>
    âœ¨ Persona Engine <img src="./assets/dance.webp" width="30px" alt="Dancing Mascot"> âœ¨
  </h1>

  <p><i>Unleash your digital characters with the magic of AI-driven voice, animation, and personality!</i></p>

<p>
  <a href="https://github.com/fagenorn/handcrafted-persona-engine/releases/latest" target="_blank"><img alt="GitHub Downloads (all assets, all releases)" src="https://img.shields.io/github/downloads/fagenorn/handcrafted-persona-engine/total?style=for-the-badge&logo=github"></a>
  &nbsp;&nbsp; <a href="https://discord.gg/p3CXEyFtrA" target="_blank"><img alt="Discord" src="https://img.shields.io/discord/1347649495646601419?style=for-the-badge&logo=discord&logoColor=white&label=Discord&color=5865F2"></a>
  &nbsp;&nbsp; <a href="https://x.com/fagenorn" target="_blank"><img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/fagenorn?style=for-the-badge&logo=x"></a>
</p>

  ---

  <p>
    <b>Persona Engine</b> is your all-in-one toolkit for creating captivating, interactive avatars! It masterfully combines:
    <br>
    ğŸ¨ <b>Live2D:</b> For expressive, real-time character animation.
    <br>
    ğŸ§  <b>Large Language Models (LLMs):</b> Giving your character a unique voice and personality.
    <br>
    ğŸ¤ <b>Automatic Speech Recognition (ASR):</b> To understand voice commands and conversation.
    <br>
    ğŸ—£ï¸ <b>Text-to-Speech (TTS):</b> Enabling your character to speak naturally.
    <br>
    ğŸ­ <b>Real-time Voice Cloning (RVC - Optional):</b> To mimic specific voice characteristics.
    <br>
    <br>
    Perfectly suited for <b>VTubing ğŸ¬</b>, dynamic <b>streaming ğŸ®</b>, and innovative <b>virtual assistant applications ğŸ¤–</b>.
    <br>
    <i>Let's bring your character to life like never before!</i> âœ¨
  </p>

Â  Â  <img src="assets/header.png" alt="Persona Engine" width="650">

Â  Â  <h2>ğŸ’– See it in Action! ğŸ’–</h2>
Â  Â  <p>Witness the Persona Engine creating digital magic:</p>
    <a href="https://www.youtube.com/watch?v=4V2DgI7OtHE" target="_blank"> <img src="assets/demo_1.png" alt="Persona Engine Demo Video" width="600"></a>
    <p><i>(Click the image above to watch the demo!)</i></p>
</div>

## ğŸ“œ Table of Contents

* [ğŸŒ¸ Overview: What's Inside?](#overview)
* [ğŸš€ Getting Started: Installation Guide](#installation-guide)
* [âœ¨ Features Galore!](#features)
* [âš™ï¸ Architecture / How it Works](#architecture)
* [ğŸ’¡ Potential Use Cases](#use-cases)
* [ğŸ¤ Contributing](#contributing)
* [ğŸ­ Live2D Integration Guide](./Live2D.md)
* [ğŸ’¬ Join Our Community!](#community)
* [â“ Support & Contact](#support)

## <a id="overview"></a>ğŸŒ¸ Overview: What's Inside?

Persona Engine listens to your voice ğŸ¤, thinks using powerful AI language models ğŸ§  (guided by a personality you define!), speaks back with a synthesized voice ğŸ”Š (which can optionally be cloned using RVC!), and animates a Live2D avatar ğŸ­ accordingly.

It integrates seamlessly into streaming software like OBS Studio using Spout for high-quality visual output. The included "Aria" model is specially rigged for optimal performance, but you can integrate your own Live2D models (see the [Live2D Integration Guide](./Live2D.md)).

> [!IMPORTANT]  
> Persona Engine achieves the most natural and in-character interactions when used with a **specially fine-tuned Large Language Model (LLM)**. This model is trained to understand the engine's specific communication format.
>
> While you *can* use standard OpenAI-compatible models (e.g., from Ollama, Groq, OpenAI), it requires careful **prompt engineering** within the `personality.txt` file. We provide a template (`personality_example.txt`) in the repository to guide you.
>
> **Detailed instructions for configuring `personality.txt` for standard models are crucial and can be found in the [Installation Guide](./INSTALLATION.md#configure-personality-txt---important).**
>
> ğŸ‘‰ Want to try the fine-tuned model or see a live demo? Hop into our [Discord](#community)! ğŸ˜Š

## <a id="installation-guide"></a>ğŸš€ Getting Started: Installation Guide


â¡ï¸ **Please follow the detailed [Installation and Setup Guide](./INSTALLATION.md) to install prerequisites, download models, configure, and run the engine.** â¬…ï¸

**Key Requirements Covered:**
* **System:** **Mandatory NVIDIA GPU with CUDA support is required** for core features (ASR, TTS, RVC).
* **Software:** .NET Runtime, espeak-ng.
* **AI Models:** Downloading Whisper ASR models.
* **Live2D:** Setting up your Live2D model (using Aria or your own).
* **(Optional) RVC:** Real-time Voice Cloning setup.
* **LLM:** Configuring access to your chosen LLM (API keys, endpoints).
* **Streaming:** Setting up Spout output for OBS/other software.
* **Configuration:** Understanding `appsettings.json`.
* **Troubleshooting:** Common issues and solutions.

## <a id="features"></a>âœ¨ Features Galore!

<div align="center">
<img src="assets/mascot_wand.png" width="150" alt="Mascot with Wand">
</div>

* ğŸ­ **Live2D Avatar Integration:**
    * Loads and renders Live2D models (`.model3.json`).
    * Includes the specially rigged "Aria" model.
    * Supports emotion-driven animations (`[EMOTION:name]`) and VBridger-standard lip-sync parameters.
    * Dedicated services for **Emotion**, **Idle**, and **Blinking** animations.
    * **See the detailed [Live2D Integration & Rigging Guide](./Live2D.md) for custom model requirements!**

* ğŸ§  **AI-Driven Conversation:**
    * Connects to OpenAI-compatible Large Language Model (`LLM`) APIs (local or cloud).
    * Guided by your custom `personality.txt` file.
    * Features **improved conversation context** and **session management** for more robust interactions.
    * Optimized for the optional special fine-tuned model (see [Overview](#overview)).

* ğŸ—£ï¸ **Voice Interaction (Requires NVIDIA `GPU`):**
    * Listens via microphone (using `NAudio`/`PortAudio`).
    * Detects speech segments using Silero `VAD`.
    * Understands speech using Whisper `ASR` (via `Whisper.NET`).
    * Includes dedicated **Barge-In Detection** to handle user interruptions more gracefully.
    * Uses a small, fast Whisper model for interruption detection and a larger, more accurate model for transcription.

* ğŸ”Š **Advanced Text-to-Speech (`TTS`) (Requires NVIDIA `GPU`):**
    * Sophisticated pipeline: Text Normalization -> Sentence Segmentation -> Phonemization -> `ONNX` Synthesis.
    * Brings text to life using custom `kokoro` voice models.
    * Uses `espeak-ng` as a fallback for unknown words/symbols.

* ğŸ‘¤ **Optional Real-time Voice Cloning (`RVC`) (Requires NVIDIA `GPU`):**
    * Integrates `RVC` `ONNX` models.
    * Modifies the `TTS` voice output in real-time to sound like a specific target voice.
    * Can be disabled for performance.

* ğŸ“œ **Customizable Subtitles:**
    * Displays spoken text with configurable styling options via the `UI`.

* ğŸ’¬ **Control `UI` & Chat Viewer:**
    * Dedicated `UI` window for monitoring engine status.
    * Viewing latency metrics (LLM, TTS, Audio)
    * Live adjustment of `TTS` parameters (pitch, rate) and Roulette Wheel settings.
    * View and edit the conversation history.

* ğŸ‘€ **Screen Awareness (Experimental):**
    * Optional Vision module allows the AI to "see" and read text from specified application windows.

* ğŸ¡ **Interactive Roulette Wheel (Experimental):**
    * An optional, configurable on-screen roulette wheel for interactive fun.

* ğŸ“º **Streaming Output (`Spout`):**
    * Sends visuals (Avatar, Subtitles, Roulette Wheel) directly to OBS Studio or other `Spout`-compatible software.
    * Uses separate, configurable `Spout` streams (no window capture needed!).

* ğŸ¶ **Audio Output:**
    * Plays generated speech clearly via `PortAudio`.

* âš™ï¸ **Configuration:**
    * Primary setup via `appsettings.json` (details in [Installation Guide](./INSTALLATION.md#configuration-appsettingsjson)).
    * Real-time adjustments for some settings via the Control `UI`.

* ğŸ¤¬ **Profanity Filtering:**
    * Basic keyword list + optional Machine Learning (`ML`)-based filtering for `LLM` responses.

## <a id="architecture"></a>âš™ï¸ Architecture / How it Works

Persona Engine operates in a continuous loop, bringing your character to life through these steps:

1.  **Listen:** ğŸ¤
    * A microphone captures audio.
    * A Voice Activity Detector (VAD) identifies speech segments.

2.  **Understand:** ğŸ‘‚
    * A fast Whisper model detects potential user interruptions.
    * Once speech ends, a more accurate Whisper model transcribes the full utterance.

3.  **Contextualize (Optional):** ğŸ‘€
    * If enabled, the Vision module captures text content from specified application windows.

4.  **Think:** ğŸ§ 
    * Transcribed text, conversation history, optional screen context, and rules from `personality.txt` are sent to the configured Large Language Model (LLM).

5.  **Respond:** ğŸ’¬
    * The LLM generates a text response.
    * This response may include emotion tags (e.g., `[EMOTION:ğŸ˜Š]`) or commands.

6.  **Filter (Optional):** ğŸ¤¬
    * The response is checked against profanity filters.

7.  **Speak:** ğŸ”Š
    * The Text-to-Speech (TTS) system converts the filtered text into audio.
    * It uses a `kokoro` voice model primarily.
    * It falls back to `espeak-ng` for unknown elements.

8.  **Clone (Optional):** ğŸ‘¤
    * If Real-time Voice Cloning (RVC) is enabled, it modifies the TTS audio in real-time.
    * This uses an ONNX model to match the target voice profile.

9.  **Animate:** ğŸ­
    * Phonemes extracted during TTS drive lip-sync parameters (VBridger standard).
    * Emotion tags in the LLM response trigger corresponding Live2D expressions or motions.
    * Idle animations play when the character is not speaking to maintain a natural look.
    * **(See [Live2D Integration & Rigging Guide](./Live2D.md) for details!)**

10. **Display:**
    * ğŸ“œ Subtitles are generated from the spoken text.
    * ğŸ“º The animated avatar, subtitles, and optional Roulette Wheel are sent via dedicated Spout streams to OBS or other software.
    * ğŸ¶ The synthesized (and optionally cloned) audio is played through the selected output device.

11. **Loop:**
    * The engine returns to the listening state, ready for the next interaction.

<div align="center">
<br/>
<img src="assets/diagram.png" width="500" alt="Diagram">
<br/>
<br/>
</div>

## <a id="use-cases"></a>ğŸ’¡ Potential Use Cases: Imagine the Fun!

<div align="center">
<img src="assets/mascot_light.png" width="150" alt="Mascot with Lightbulb">
</div>

* ğŸ¬ **VTubing & Live Streaming:** Create an AI co-host, an interactive character responding to chat, or even a fully AI-driven VTuber persona.
* ğŸ¤– **Virtual Assistant:** Build a personalized, animated desktop companion that talks back.
* ğŸª **Interactive Kiosks:** Develop engaging virtual guides for museums, trade shows, retail environments, or information points.
* ğŸ“ **Educational Tools:** Design an AI language practice partner, an interactive historical figure Q&A bot, or a dynamic tutor.
* ğŸ® **Gaming:** Implement more dynamic and conversational Non-Player Characters (NPCs) or companion characters in games.
* ğŸ’¬ **Character Chatbots:** Allow users to have immersive conversations with their favorite fictional characters brought to life.

## <a id="contributing"></a>ğŸ¤ Contributing

Contributions are highly welcome! If you have improvements, bug fixes, or new features in mind, please follow these steps:

1.  **Discuss (Optional but Recommended):** For major changes, please open a [GitHub Issue](https://github.com/fagenorn/handcrafted-persona-engine/issues) first to discuss your ideas.
2.  **Fork:** Fork the repository to your own GitHub account.
3.  **Branch:** Create a new feature branch for your changes (`git checkout -b feature/YourAmazingFeature`).
4.  **Code:** Make your changes. Please try to adhere to the existing coding style and add comments where necessary.
5.  **Commit:** Commit your changes with clear messages (`git commit -m 'Add some AmazingFeature'`).
6.  **Push:** Push your branch to your fork (`git push origin feature/YourAmazingFeature`).
7.  **Pull Request:** Open a Pull Request (PR) back to the `main` branch of the original repository. Describe your changes clearly in the PR.

Your help in making Persona Engine better is greatly appreciated! ğŸ˜Š

## <a id="community"></a>ğŸ’¬ Join Our Community!

<div align="center">
<p>
Need help getting started? Have questions or brilliant ideas? ğŸ’¡ Want to see a live demo, test the special fine-tuned model, or chat directly with a Persona Engine character? Having trouble converting RVC models or rigging your own Live2D model? Come say hi on Discord! ğŸ‘‹
</p>
<a href="https://discord.gg/p3CXEyFtrA" target="_blank">
<img src="assets/discord.png" alt="Join Discord Img"
Â  width="400"
Â  /></a>
Â  <br>
<a href="https://discord.gg/p3CXEyFtrA" target="_blank">
<img src="https://img.shields.io/discord/1347649495646601419?label=Join%20Discord&logo=discord&style=for-the-badge" alt="Join Discord Badge" />
</a>
<br>
<br>
<p>You can also report bugs or request features via <a href="https://github.com/fagenorn/handcrafted-persona-engine/issues" target="_blank">GitHub Issues</a>.</p>
</div>

## <a id="support"></a>â“ Support & Contact

* **Primary Support & Community:** Please join our [Discord Server](#community) for help, discussion, and demos.
* **Bug Reports & Feature Requests:** Please use [GitHub Issues](https://github.com/fagenorn/handcrafted-persona-engine/issues).
* **Direct Contact:** You can also reach out via [Twitter/X](https://x.com/fagenorn).

-----

> [!TIP]
> *Remember to consult the [Live2D Integration & Rigging Guide](./Live2D.md) for details on preparing custom avatars.*
> *For detailed setup steps, please refer to the [Installation and Setup Guide](./INSTALLATION.md).*
